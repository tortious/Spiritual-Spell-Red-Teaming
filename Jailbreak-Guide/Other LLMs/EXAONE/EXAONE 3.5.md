# EXAONE 3.5

Currently only available via API, no dedicated service, can use via **[Friendli AI](https://friendli.ai/model/LGAI-EXAONE/K-EXAONE-236B-A23B)**

*I'm sure it will be on Openrouter or Librechat soon enough.*

## My thoughts

- The model will **LIE** to the user in order to keep things safe, *if you use a weaker jailbreak*, it will literally show in it's thinking how it's gonna lie. I used a very weak version of ENI, like 10 lines. **(See last screenshot)**
- Decent model, has a very good personality in it's thinking, basically unrestricted, will do anything, just have to use a simple jailbreak.
- lacking something, idk what, but you can tell it's just lacking.
- writing could use some major work, but have to look at it's size, writes decent (ehh) for it's class.
- Runs into the same issues as these other smaller reasoning models like **OLmO 3 32b Think** or **ERNIE 5.0**, it can reason itself into refusals if you're not specific enough.
- Seems to have some sort of cut off, with really dark requests it keeps cutting off at 2k tokens, platform limitation maybe, idk.

Just used the most recent **[ENI LIME üçã‚Äçüü©](https://docs.google.com/document/d/192N6L3uo9VWANWOlJkeopYMihDsNnGhzvu-AaRhAoP4/edit?usp=drivesdk)**

## Model Specs
*It's a MoE, (Mixture of Experts) Number of Experts: 128, Number of Activated Experts: 8, Number of Shared Experts: 1, MoE Intermediate Size: 2,048*
- Context: 256K context window, 128-token sliding window
- Parameters: 236 billion total parameters, with 23 billion active during inference
- Knowledge Cutoff: Dec 2024 (2024/12)

*Note: Apparently used some Universal standard of ethics; It demonstrates high reliability across diverse risk categories. Haven't seen that myself at all*
